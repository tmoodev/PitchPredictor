{"paragraphs":[{"text":"%pyspark\nfrom pyspark.sql import SparkSession\nfrom pybaseball import statcast\nfrom pyspark.sql.functions import col, count, isnan, when, mean, stddev\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder\nimport pyarrow.parquet as pq\n\n\n#Start Spark Session w. Memory Parameters\nspark = SparkSession.builder \\\n    .appName(\"Baseball\") \\\n    .master(\"spark://10.139.0.200:7077\") \\\n    .getOrCreate()\n\n# Check if the SparkContext is alive\nprint(\"Spark UI:\", spark.sparkContext.uiWebUrl)\nprint(\"Master:\", spark.sparkContext.master)\nprint(\"Executors:\", spark.sparkContext._jsc.sc().statusTracker().getExecutorInfos())\n","user":"travis","dateUpdated":"2025-02-25T19:49:15+0000","progress":0,"config":{"colWidth":12,"fontSize":9,"results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1740512955225_780331612","id":"paragraph_1739814427023_953545793","dateCreated":"2025-02-25T19:49:15+0000","status":"READY"},{"text":"%pyspark\n#Ingest raw parquet into Spark df\n\ntable = pq.read_table(\"/home/travis/statcast_2015_2024.parquet\")\npq.write_table(table, \n               \"statcast_2015_2024_converted.parquet\",\n               version=\"2.6\",   # Parquet format version\n               flavor=\"spark\")  # helps keep Spark compatibility\n","user":"travis","dateUpdated":"2025-02-25T19:49:15+0000","progress":0,"config":{"colWidth":12,"fontSize":9,"results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1740512955227_978312664","id":"paragraph_1739902686279_1824862283","dateCreated":"2025-02-25T19:49:15+0000","status":"READY"},{"text":"%pyspark\nraw_df = spark.read.parquet(\"statcast_2015_2024_converted.parquet\")\nraw_df.show(5)","user":"travis","dateUpdated":"2025-02-25T19:49:15+0000","progress":0,"config":{"colWidth":12,"fontSize":9,"results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1740512955227_761440643","id":"paragraph_1739909681175_882847396","dateCreated":"2025-02-25T19:49:15+0000","status":"READY"},{"text":"%pyspark\n\n# Attempting to free up some space\n\nimport gc\ndel table\ngc.collect()","user":"travis","dateUpdated":"2025-02-25T19:49:15+0000","progress":0,"config":{"colWidth":12,"fontSize":9,"results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1740512955229_787972394","id":"paragraph_1740501822131_1986412424","dateCreated":"2025-02-25T19:49:15+0000","status":"READY"},{"text":"%pyspark\nfrom pyspark.sql.functions import monotonically_increasing_id\n#Add an pk to the dataframe\ndf = raw_df.withColumn(\"id\", monotonically_increasing_id())","user":"travis","dateUpdated":"2025-02-25T19:49:15+0000","progress":0,"config":{"colWidth":12,"fontSize":9,"results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1740512955229_1778487354","id":"paragraph_1739905991686_1084169391","dateCreated":"2025-02-25T19:49:15+0000","status":"READY"},{"text":"%pyspark\n# Remove deprecated columns\n\ncolumns_to_drop = [\n    \"spin_rate_deprecated\", \"break_angle_deprecated\", \"break_length_deprecated\",\n    \"tfs_deprecated\", \"tfs_zulu_deprecated\"\n]\n\ncleaned_df = df.drop(*columns_to_drop)\n\ncleaned_df.printSchema()","user":"travis","dateUpdated":"2025-02-25T19:49:15+0000","progress":0,"config":{"colWidth":12,"fontSize":9,"results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1740512955229_1345065344","id":"paragraph_1739896110342_456195083","dateCreated":"2025-02-25T19:49:15+0000","status":"READY"},{"text":"%pyspark\n#Identify numeric columns and count NaN values\n\n# Get all numeric columns \nnumeric_cols = [f.name for f in cleaned_df.schema.fields if f.dataType.simpleString() in [\"int\", \"bigint\", \"double\", \"float\"]]\n\n# Count NaN values in all numeric columns\nnan_counts = cleaned_df.select(\n    [count(when(isnan(col(c)), c)).alias(c) for c in numeric_cols]\n)\n\nnan_counts.show()\n","user":"travis","dateUpdated":"2025-02-25T19:49:15+0000","progress":0,"config":{"colWidth":12,"fontSize":9,"results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1740512955230_1130782025","id":"paragraph_1739896752390_1897148345","dateCreated":"2025-02-25T19:49:15+0000","status":"READY"},{"text":"%pyspark\n\n# Count total rows in DataFrame\ntotal_rows = cleaned_df.count()\n\n# Count NaN and NULL values for each numeric column\nnan_counts = cleaned_df.select([\n    count(when(isnan(col(c)) | col(c).isNull(), c)).alias(c) for c in numeric_cols\n]).collect()[0].asDict()\n\n# Identify columns with more than 50% NaN values\nthreshold = 0.5 * total_rows\ncolumns_to_drop = [col_name for col_name, nan_count in nan_counts.items() if nan_count > threshold]\n\n# Drop columns with excessive missing values\ncleaned_df = cleaned_df.drop(*columns_to_drop)\n\n# Show the dropped columns\nprint(f\"Dropped columns: {columns_to_drop}\")\nprint(cleaned_df.count())\n\n\n","user":"travis","dateUpdated":"2025-02-25T19:49:15+0000","progress":0,"config":{"colWidth":12,"fontSize":9,"results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1740512955231_133584039","id":"paragraph_1739897394071_1758472772","dateCreated":"2025-02-25T19:49:15+0000","status":"READY"},{"text":"%pyspark\ncleaned_df.printSchema()","user":"travis","dateUpdated":"2025-02-25T19:49:15+0000","progress":0,"config":{"colWidth":12,"fontSize":9,"results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1740512955232_577033150","id":"paragraph_1739909971664_407757708","dateCreated":"2025-02-25T19:49:15+0000","status":"READY"},{"text":"%pyspark\n############################\n# Identify & Remove Outliers\n############################\n\nfrom pyspark.sql.functions import col, mean, stddev\n\n# Filter by domain knowledge for pitch speed:\ndf_no_outliers = cleaned_df.filter(\n    (cleaned_df[\"release_speed\"] >= 40) & \n    (cleaned_df[\"release_speed\"] <= 105)\n)\n\n# Filter by standard deviation:\nstats = (\n    df_no_outliers\n    .select(\n        mean(col(\"release_speed\")).alias(\"mean_speed\"),\n        stddev(col(\"release_speed\")).alias(\"std_speed\")\n    )\n    .collect()[0]\n)\n\nmean_speed = stats[\"mean_speed\"]\nstd_speed = stats[\"std_speed\"]\n\nlow_cutoff = mean_speed - 3 * std_speed\nhigh_cutoff = mean_speed + 3 * std_speed\n\ndf_no_outliers = df_no_outliers.filter(\n    (col(\"release_speed\") >= low_cutoff) &\n    (col(\"release_speed\") <= high_cutoff)\n)\n\n# Reduce the number of columns to lighten df:\nfiltered_for_iqr = df_no_outliers.select(\n    \"id\",\n    \"release_spin_rate\",\n    \"plate_x\",\n    \"plate_z\",\n    \"release_extension\",\n    \"api_break_x_batter_in\",\n    \"api_break_x_arm\",\n    \"api_break_z_with_gravity\",\n    \"delta_pitcher_run_exp\",\n    \"spin_axis\",\n    \"pitch_number\",\n    \"release_pos_y\",\n    \"release_speed\",\n    \"release_pos_x\",\n    \"release_pos_z\",\n    \"zone\",\n    \"pfx_x\",\n    \"pfx_z\",\n    \"vx0\",\n    \"vy0\",\n    \"vz0\",\n    \"ax\",\n    \"ay\",\n    \"az\",\n    \"sz_top\",\n    \"sz_bot\",\n)\n\nfiltered_for_iqr.show(5)\n","user":"travis","dateUpdated":"2025-02-25T19:49:15+0000","progress":0,"config":{"colWidth":12,"fontSize":9,"results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1740512955233_181083559","id":"paragraph_1739896018036_37910902","dateCreated":"2025-02-25T19:49:15+0000","status":"READY"},{"text":"%pyspark\n\nnumeric_cols = [f.name for f in filtered_for_iqr.schema.fields if f.dataType.simpleString() in [\"int\", \"bigint\", \"double\", \"float\"]]\n\n","user":"travis","dateUpdated":"2025-02-25T19:49:15+0000","progress":0,"config":{"tableHide":false,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1740512955233_621056660","id":"paragraph_1740503929860_921758210","dateCreated":"2025-02-25T19:49:15+0000","status":"READY"},{"text":"%pyspark\n\nconditions = []\nfor c in numeric_cols:\n    Q1, Q3 = filtered_for_iqr.approxQuantile(c, [0.25, 0.75], 0.05)  # allow small error for speed/memory\n    if len([Q1, Q3]) == 2:\n        IQR = Q3 - Q1\n        lower_bound = Q1 - 1.5 * IQR\n        upper_bound = Q3 + 1.5 * IQR\n        conditions.append((col(c) >= lower_bound) & (col(c) <= upper_bound))\n\ncombined_condition = conditions[0]\nfor cond in conditions[1:]:\n    combined_condition = combined_condition & cond\n\noutliers_removed = filtered_for_iqr.filter(combined_condition)\noutliers_removed.show(5)\nrf_sql = outliers_removed\noutliers_removed.unpersist()\nfiltered_for_iqr.unpersist()","user":"travis","dateUpdated":"2025-02-25T19:49:15+0000","progress":0,"config":{"colWidth":12,"fontSize":9,"results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1740512955234_820507117","id":"paragraph_1739910025117_132960677","dateCreated":"2025-02-25T19:49:15+0000","status":"READY"},{"text":"%pyspark\n\n#This guy is no bueno. Big memory suck\n\nfor c in numeric_cols:\n    quantiles = filtered_for_iqr.approxQuantile(c, [0.25, 0.75], 0.0)\n    \n    # Ensure two quantile values exist before proceeding\n\n    if len(quantiles) == 2:\n        Q1, Q3 = quantiles\n        IQR = Q3 - Q1\n        lower_bound = Q1 - 1.5 * IQR\n        upper_bound = Q3 + 1.5 * IQR\n        \n        # Apply filtering to remove outliers\n\n        outliers_removed = filtered_for_iqr.filter((col(c) >= lower_bound) & (col(c) <= upper_bound))\n    else:\n        print(f\"Skipping column '{c}' due to insufficient quantile data.\")\n\n# Show the cleaned DataFrame\n\noutliers_removed.show(5) \n\nfor c in numeric_cols:\n    quantiles = filtered_for_iqr.approxQuantile(c, [0.25, 0.75], 0.0)\n    \n    # Ensure two quantile values exist before proceeding\n\n    if len(quantiles) == 2:\n        Q1, Q3 = quantiles\n        IQR = Q3 - Q1\n        lower_bound = Q1 - 1.5 * IQR\n        upper_bound = Q3 + 1.5 * IQR\n        \n        # Apply filtering to remove outliers\n\n        outliers_removed = filtered_for_iqr.filter((col(c) >= lower_bound) & (col(c) <= upper_bound))\n    else:\n        print(f\"Skipping column '{c}' due to insufficient quantile data.\")\n\n# Show the cleaned DataFrame\n\n\n","user":"travis","dateUpdated":"2025-02-25T19:49:15+0000","progress":0,"config":{"colWidth":12,"fontSize":9,"results":{},"enabled":false,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1740512955235_591626779","id":"paragraph_1740504180845_364944890","dateCreated":"2025-02-25T19:49:15+0000","status":"READY"},{"text":"%md\n\n\n\nWhen you iterate over each numeric column and repeatedly run approxQuantile and filter inside the same loop, Spark ends up creating multiple plans and holding intermediate DataFrames in memory. A few tips to reduce the spike in memory usage:\n\n## Filter Once per Column or Build a Single Condition\nCurrently, you filter your data inside the loop for each column individually:\noutliers_removed = filtered_for_iqr.filter((col(c) >= lower_bound) & (col(c) <= upper_bound))\nThis means you are repeatedly applying filters, and each DataFrame transformation is still tied to filtered_for_iqr. By default, Spark will hold onto the full query plan history to maintain lineage.\n\nOne strategy is to build a single combined filter condition over all numeric columns, and then run one .filter() operation. For example:\n\nconditions = []\nfor c in numeric_cols:\n    Q1, Q3 = filtered_for_iqr.approxQuantile(c, [0.25, 0.75], 0.05)  # allow small error for speed/memory\n    if len([Q1, Q3]) == 2:\n        IQR = Q3 - Q1\n        lower_bound = Q1 - 1.5 * IQR\n        upper_bound = Q3 + 1.5 * IQR\n        conditions.append((col(c) >= lower_bound) & (col(c) <= upper_bound))\n\ncombined_condition = conditions[0]\nfor cond in conditions[1:]:\n    combined_condition = combined_condition & cond\n\noutliers_removed = filtered_for_iqr.filter(combined_condition)\noutliers_removed.show(5)\nThis way, you only run one filter pass on the DataFrame instead of re-filtering each time through the loop. It will reduce lineage size and help keep memory usage under control.\n\n## Use an Approximate Error\nYou set the relative error to 0.0 in approxQuantile(c, [0.25, 0.75], 0.0). This forces Spark to be as precise as possible and can significantly increase computation and memory usage. Consider letting Spark approximate the quartiles by setting a small error term, like 0.05:\n\nQ1, Q3 = filtered_for_iqr.approxQuantile(c, [0.25, 0.75], 0.05)\nWith real-world datasets, a slight approximation usually doesn’t harm the analysis and helps reduce memory spikes.\n\n\nBy default, Spark does lazy evaluation. If you cache or persist a DataFrame, it can remain in memory across actions. So be mindful of what you persist.\nAvoid Creating Extra DataFrames in Each Iteration\nIn your loop, you do:\noutliers_removed = filtered_for_iqr.filter(...)\nfor every column. Spark’s lineage engine will store the transformations in memory for each step until an action (like show()) triggers execution. If you need a final single DataFrame, don’t overwrite outliers_removed inside the loop repeatedly—either build that single filter once, or do your computations on the driver (e.g., computing quartiles for each column first) and then filter once at the end.\n\n## Verify the Size of the Dataset\nIf you’re dealing with a massive dataset (potentially hundreds of millions of rows), each pass over the data can blow up memory usage. Make sure you truly need to remove outliers across every numeric column, or consider sampling if you only need to see distribution patterns.\n\n## Avoid Forcing Full Materialization\nCalling .show() on a huge DataFrame can trigger a job that tries to collect a large portion of the data into the Spark UI or driver memory. Even though .show(5) only prints five rows, Spark must still run a full job to figure out which five records pass all the filtering steps. One solution is to break up the pipeline or use a limit() earlier to reduce overhead in testing stages.\n\n","user":"travis","dateUpdated":"2025-02-25T19:49:15+0000","progress":0,"config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1740512955235_1162280038","id":"paragraph_1740503987975_2130603677","dateCreated":"2025-02-25T19:49:15+0000","status":"READY"},{"text":"%pyspark\n#Rejoin dataset with outliers removed to attributes from df_cleaned\n#Create temp sql views\nrf_sql.createOrReplaceTempView(\"removed_outliers\")\ncleaned_df.createOrReplaceTempView(\"cleaned\")\n\nready_for_index = spark.sql(\"\"\"\nSELECT *\nFROM removed_outliers a\nLEFT JOIN cleaned b\n    ON a.id = b.id\n\"\"\")\n\nready_for_index.show()","user":"travis","dateUpdated":"2025-02-25T19:49:15+0000","progress":0,"config":{"colWidth":12,"fontSize":9,"results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{},"progressUpdateIntervalMs":500,"jobName":"paragraph_1740512955237_574949719","id":"paragraph_1739906871712_1904522248","dateCreated":"2025-02-25T19:49:15+0000","status":"READY"},{"text":"%pyspark\n\n#Index the categorical pitch_type column\npitch_type_indexer = StringIndexer(\n    inputCol=\"pitch_type\", \n    outputCol=\"pitch_type_index\"\n)\n\n# One-hot encode the indexed pitch_type\npitch_type_encoder = OneHotEncoder(\n    inputCols=[\"pitch_type_index\"], \n    outputCols=[\"pitch_type_ohe\"]\n)\n\n# Fit the indexer on df_filtered and transform\nindexed_df = pitch_type_indexer.fit(ready_for_index).transform(ready_for_index)\n\n# Fit the encoder on the indexed data and transform\nmodel_data = pitch_type_encoder.fit(indexed_df).transform(indexed_df)\n\n# --> encoded_df now contains a new vector column 'pitch_type_ohe'\n#     that you can use downstream in modeling.\n\n","user":"travis","dateUpdated":"2025-02-25T19:49:42+0000","progress":0,"config":{"colWidth":12,"fontSize":9,"results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true}},"settings":{"params":{},"forms":{}},"apps":[],"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":[{"jobUrl":"http://10.139.0.200:4040/jobs/job?id=54"},{"jobUrl":"http://10.139.0.200:4040/jobs/job?id=55"},{"jobUrl":"http://10.139.0.200:4040/jobs/job?id=56"},{"jobUrl":"http://10.139.0.200:4040/jobs/job?id=57"}],"interpreterSettingId":"spark"}},"progressUpdateIntervalMs":500,"jobName":"paragraph_1740512955241_2075991753","id":"paragraph_1739898511715_727620","dateCreated":"2025-02-25T19:49:15+0000","dateStarted":"2025-02-25T19:49:42+0000","dateFinished":"2025-02-25T19:49:53+0000","status":"FINISHED"}],"name":"PitchPredictor_rf_index","id":"2KQR4DQ6A","defaultInterpreterGroup":"spark","version":"0.12.0","noteParams":{},"noteForms":{},"angularObjects":{},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{},"path":"/PitchPredictor_rf_index"}